

Respostas 

Trabalho – Classificação Binária de Textos com LIME e SHAP 

 

Grupo: 

Antoniel William 

Filipe Neiva 

Kleberson Vilela 

Lucas Coelho 

Lucas Farias 

 

 

1. Por que o pré-processamento textual é importante para o desempenho do modelo? 

 

O pré-processamento remove ruídos e padroniza os textos, tornando-os mais adequados para análise.Operações como remoção de stopwords, normalização e limpeza de caracteres especiais reduzem a complexidade e evitam que o modelo aprenda padrões irrelevantes. Isso melhora a generalização e aumenta a eficiência do treinamento, resultando em modelos mais precisos e rápidos. 

 

2. Qual a diferença fundamental entre BOW e TF-IDF? Dê um exemplo simples. 

 

Bag-of-words(BOW): conta a frequência das palavras em um documento, sem considerar sua importância relativa. 

TF-IDF: além da frequência, pondera a relevância da palavra considerando o corpus. Palavras muito comuns (como “o”, “de”) recebem peso baixo, enquanto palavras mais específicas recebem peso alto. 

 

Exemplo: Documento 1: “gato preto”; Documento 2: “gato branco”. No BOW, “gato” teria peso igual nos dois documentos. No TF-IDF, se “gato” aparece em quase todos os textos, seu peso será menor, enquanto “preto” ou “branco” terão maior relevância por distinguirem os documentos. 

 

3. Em que situações TF-IDF tende a ser mais vantajoso do que BOW? 

 

Quando queremos diferenciar documentos por termos mais específicos e informativos e em tarefas como classificação de textos, recuperação de informação e busca, pois o TF-IDF destaca termos que realmente caracterizam o conteúdo, enquanto o BOW pode dar peso excessivo a palavras comuns e pouco informativas. 

 

4. O que é um vetor esparso e por que isso é comum em problemas de PLN? 

 

 

5. Por que modelos lineares podem funcionar bem em textos com alta dimensionalidade? 

 

 

 

6. Quais são as principais diferenças entre um pipeline de classificação com dados tabulares e um de textos? 

 

 

 

7. O que significa interpretabilidade/explicabilidade de um modelo? 

 

 

 

8. Qual a diferença entre explicação local e explicação global? 

 

A explicação local busca mostrar quais features influenciaram a previsão de uma instância específica, explicando por que o modelo tomou aquela decisão pontual. 

A explicação global resume o comportamento do modelo como um todo, mostrando a importância média das features ao longo de todo o dataset e como elas influenciam as predições de forma geral. 

 

9. Por que precisamos de LIME e SHAP mesmo usando modelos lineares? 

 

Mesmo sendo modelos lineares(voltados pra classificação), quando na condição de possuir muitas features, isso fica mais complexo de interpretar o modo como o modelo classifica. Por isso, se utiliza lime e shap para ter uma melhor compreensão por meio da influência das features para casos específicos ou para o modelo como um todo. 

 

10. Como o LIME gera explicações locais? Qual o papel das perturbações da instância? 

 

Criando pertubações, ou seja, treinando o modelo com outras palavras, retirando features e observando como o modelo altera sua previsão. Depois ele treina o modelo linear simples sobre essas instâncias perturbadas, ponderadas pela proximidade com a amostra original. 

 

11. Como o SHAP calcula a importância das palavras? Qual é a ideia básica por trás dos 

valores de Shapley? 

O SHAP calcula a importância das palavras usando os valores de Shapley, que vêm da Teoria dos Jogos. 
A ideia central é avaliar quanto cada feature contribui para a diferença entre a previsão atual e a previsão média, considerando todas as combinações possíveis de features entrando ou saindo do modelo. 
Assim, cada palavra recebe um valor SHAP que indica sua contribuição individual e justa para a predição. 

 

12. Como interpretar um summary plot do SHAP em um problema de texto? 

 

O summary plot segue essa estrutura: tem o eixo com as features(palavras). No meio do gráfico, tem uma linha que vai fazer esse limite entre as classes. Para cada feature, tem uma barra colorida de uma determinada cor e intensidade, indicando o quanto ela contribui na classificação. As features ficam no eixo vertical e os valores SHAP ficam no eixo horizontal. 
 
Features que estão do lado direito da linha, classificam para a classe positiva. As que estão à esquerda, geralmente classificam para negativa. 

 

13. Em que situações as explicações do LIME e do SHAP concordaram no experimento de vocês ? 

 

Eles concordaram nas explicações locais, quando as principais palavras destacadas por ambos os métodos mostravam importância semelhante e apontavam para a mesma direção na classificação da instância analisada. 

 

 

14. Em que situações elas discordaram? Qual hipótese vocês têm para essa diferença ? 

Houve discordância em alguns casos individuais, como no exemplo em que a palavra "play" teve maior contribuição no SHAP, enquanto "ac" apareceu mais relevante no LIME. 
Essa diferença pode ocorrer porque os métodos usam critérios distintos: 

O LIME depende das perturbações locais e do modelo linear aproximado. 

O SHAP usa valores de Shapley, considerando todas as combinações possíveis de features. 

 

15. Houve alguma palavra considerada importante pelo modelo que parecia não fazer 

sentido para vocês? O que isso pode indicar ? 

 

Sim, a palavra "ac" no SHAP local do exemplo 10. 
 

Isso pode indicar: ruído no texto, correlações espúrias aprendidas pelo modelo, algum padrão do dataset que não é intuitivo para humanos ou até problemas no pré-processamento dos dados. 

16. Como vocês construíram uma explicação global a partir de um método local como o LIME ? 

 

O LIME, por si só, é um método local. Para obter uma explicação global com ele, é preciso agregar várias explicações locais, por exemplo: gerar explicações para muitas instâncias e somar/contar a frequência das palavras mais importantes. 

No experimento, porém, a explicação global foi feita usando o SHAP, que já fornece nativamente summary plots e importâncias globais, enquanto o LIME ficou responsável pelas explicações locais. 
